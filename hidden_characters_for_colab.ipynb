{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tX4qkH_B_LWe"
   },
   "source": [
    "Hi! Welcome to the official colab demo for our demo \"Diffusion Illusions: Hiding Images in Plain Sight\". [https://ryanndagreat.github.io/Diffusion-Illusions/](https://ryanndagreat.github.io/Diffusion-Illusions/)\n",
    "\n",
    "This project was inspired by our paper \"Peekaboo: Text to Image Diffusion Models are Zero-Shot Segmentors\". The Peekaboo project website: [https://ryanndagreat.github.io/peekaboo/](https://ryanndagreat.github.io/peekaboo/)\n",
    "\n",
    "Instructions:\n",
    "\n",
    "0. Go to the Runtime menu, and make sure this notebook is using GPU!\n",
    "1. Run the top 2 code cells (one cleans colab's junk and downloads the source code, while the other installs python packages)\n",
    "2. Click 'Runtime', then 'Restart Runtime'. You need to do this the first time you open this notebook to avoid weird random errors from the pip installations.\n",
    "3. Run code cells to load stable diffusion. The first time you run it it will take a few minutes to download; subsequent times won't take long at all though.\n",
    "4. Run all the cells below that, and customize prompt_w, prompt_x, prompt_y, and prompt_z!\n",
    "5. Take the result top_image and bottom_image, print them out, and shine a backlight through them like shown in the Diffusion Illusion website (link above!)\n",
    "\n",
    "I may also create a YouTube tutorial if there's interest. Let me know if this would be helpful!\n",
    "\n",
    "This notebook was written by Ryan Burgert. Feel free to reach out to me at rburgert@cs.stonybrook.edu if you have any questions! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "80riJZ7f_LyL"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ ! -d \".git\" ]; then \n",
    "    rm -rf * .*; #Get rid of Colab's default junk files\n",
    "    git clone -b master https://github.com/RyannDaGreat/Diffusion-Illusions .\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aGDndq0_bch2",
    "outputId": "81a50af6-5861-4990-dfe5-871ac1682d28"
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade -r requirements.txt\n",
    "%pip install rp --upgrade\n",
    "# You may need to restart the runtime after installing these\n",
    "# I'm not sure why this helps, but all sorts of weird random errors pop up in Colab if you don't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FAtxvveUbquu"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import source.stable_diffusion as sd\n",
    "from easydict import EasyDict\n",
    "from source.learnable_textures import LearnableImageFourier\n",
    "from source.stable_diffusion_labels import NegativeLabel\n",
    "from itertools import chain\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QRCode模式：上传目标图片\n",
    "\n",
    "如果你设置了 `USE_IMAGE_FOR_Z = True`，请在这个cell中上传你的目标图片（比如QRCode）。\n",
    "\n",
    "**使用方法：**\n",
    "1. 运行下面的代码cell\n",
    "2. 点击上传按钮，选择你的图片文件\n",
    "3. 图片会被自动调整大小并转换为正确的格式\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 上传目标图片（仅在USE_IMAGE_FOR_Z=True时使用） =====\n",
    "# 注意：这个cell需要在device定义之后运行（即运行Cell 9之后）\n",
    "target_image_z = None\n",
    "\n",
    "if USE_IMAGE_FOR_Z:\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        from PIL import Image\n",
    "        import io\n",
    "        \n",
    "        print(\"请上传你的目标图片（例如QRCode）：\")\n",
    "        uploaded = files.upload()\n",
    "        \n",
    "        if uploaded:\n",
    "            # 获取上传的文件\n",
    "            file_name = list(uploaded.keys())[0]\n",
    "            image_bytes = uploaded[file_name]\n",
    "            \n",
    "            # 加载图片\n",
    "            pil_image = Image.open(io.BytesIO(image_bytes))\n",
    "            \n",
    "            # 转换为RGB格式（如果是RGBA或其他格式）\n",
    "            if pil_image.mode != 'RGB':\n",
    "                pil_image = pil_image.convert('RGB')\n",
    "            \n",
    "            # 转换为numpy数组并调整大小到256x256（与learnable_image的尺寸匹配）\n",
    "            target_image_z = np.array(pil_image)\n",
    "            target_image_z = rp.resize_image(target_image_z, (256, 256))\n",
    "            \n",
    "            # 转换为正确的格式：先确保是RGB格式，再转换为float，最后转换为torch tensor\n",
    "            # 注意：device需要在Cell 9中定义\n",
    "            if 'device' in globals():\n",
    "                # 步骤1: 确保是RGB格式（HWC格式）\n",
    "                target_image_z = rp.as_rgb_image(target_image_z)\n",
    "                # 步骤2: 转换为float格式（值在[0,1]之间），仍然是numpy数组（HWC）\n",
    "                target_image_z = rp.as_float_image(target_image_z)\n",
    "                \n",
    "                # QRCode预处理：转换为黑白（二值化），确保与黑白prompts匹配\n",
    "                # 对于QRCode，我们需要高对比度、黑白分明，与前四张黑白图片风格一致\n",
    "                if target_image_z.ndim == 3:\n",
    "                    # 步骤1: 转换为灰度图（如果QRCode是彩色的，取RGB平均值）\n",
    "                    gray = np.mean(target_image_z, axis=2, keepdims=True)\n",
    "                    gray = np.repeat(gray, 3, axis=2)\n",
    "                    \n",
    "                    # 步骤2: 归一化到[0,1]范围\n",
    "                    gray_min = gray.min()\n",
    "                    gray_max = gray.max()\n",
    "                    if gray_max > gray_min:\n",
    "                        gray_normalized = (gray - gray_min) / (gray_max - gray_min)\n",
    "                    else:\n",
    "                        gray_normalized = gray\n",
    "                    \n",
    "                    # 步骤3: 强二值化（阈值化），转换为纯黑白\n",
    "                    # 使用自适应阈值或固定阈值，确保QRCode清晰\n",
    "                    try:\n",
    "                        from skimage.filters import threshold_otsu\n",
    "                        # 尝试使用Otsu算法自动计算最佳阈值\n",
    "                        threshold = threshold_otsu(gray_normalized[:,:,0])\n",
    "                        print(f\"✓ 使用Otsu算法计算阈值: {threshold:.3f}\")\n",
    "                    except:\n",
    "                        # 如果失败，使用固定阈值\n",
    "                        threshold = 0.5\n",
    "                        print(\"✓ 使用固定阈值: 0.5\")\n",
    "                    \n",
    "                    # 强二值化：直接转换为0或1，不保留中间值\n",
    "                    target_image_z = (gray_normalized[:,:,0] > threshold).astype(np.float32)\n",
    "                    target_image_z = np.stack([target_image_z, target_image_z, target_image_z], axis=2)\n",
    "                    \n",
    "                    # 可选：形态学操作，清理噪声（可选，但有助于QRCode清晰度）\n",
    "                    try:\n",
    "                        from scipy import ndimage\n",
    "                        # 轻微的开运算，去除小噪点\n",
    "                        target_image_z = ndimage.binary_opening(target_image_z[:,:,0], structure=np.ones((2,2))).astype(np.float32)\n",
    "                        target_image_z = np.stack([target_image_z, target_image_z, target_image_z], axis=2)\n",
    "                        print(\"✓ 已应用形态学操作清理噪声\")\n",
    "                    except:\n",
    "                        pass  # 如果scipy不可用，跳过\n",
    "                    \n",
    "                    print(\"✓ QRCode已转换为纯黑白高对比度格式，与前四张图片风格匹配\")\n",
    "                \n",
    "                # 步骤3: 转换为torch tensor（CHW格式）并移动到device\n",
    "                target_image_z = rp.as_torch_image(target_image_z).to(device)\n",
    "                \n",
    "                print(f\"✓ 图片已加载，尺寸: {target_image_z.shape}\")\n",
    "                print(\"预览目标图片：\")\n",
    "                rp.display_image(rp.as_numpy_image(target_image_z))\n",
    "            else:\n",
    "                print(\"⚠️ 请先运行Cell 9来定义device变量\")\n",
    "                target_image_z = None\n",
    "                USE_IMAGE_FOR_Z = False\n",
    "        else:\n",
    "            print(\"⚠️ 未上传图片，将使用文本prompt模式\")\n",
    "            USE_IMAGE_FOR_Z = False\n",
    "    except ImportError:\n",
    "        print(\"⚠️ 不在Colab环境中，无法上传图片。将使用文本prompt模式\")\n",
    "        USE_IMAGE_FOR_Z = False\n",
    "else:\n",
    "    print(\"ℹ️ QRCode模式未启用，跳过图片上传\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x7A1Fw50eDjJ",
    "outputId": "37ceb472-da12-4c74-a76b-a89f39a3a14d"
   },
   "outputs": [],
   "source": [
    "#ONLY GOOD PROMPTS HERE\n",
    "example_prompts = rp.load_yaml_file('source/example_prompts.yaml')\n",
    "print('Available example prompts:', ', '.join(example_prompts))\n",
    "\n",
    "# ===== QRCode模式：使用图片作为prompt_z的目标 =====\n",
    "# 如果你想使用自己的图片（比如QRCode）作为overlay后的目标图像，请设置 USE_IMAGE_FOR_Z = True\n",
    "# 然后在上传图片的cell中上传你的图片\n",
    "USE_IMAGE_FOR_Z = False  # 设置为True以启用图片模式\n",
    "\n",
    "# QRCode模式：使用有语义内容但风格匹配QRCode的prompts\n",
    "# 保持illusion效果：每张图片都有独立的语义解释，但风格上强调高对比度、黑白\n",
    "# 这样既保持了\"隐藏\"的illusion效果，又能让overlay后清晰显示QRCode\n",
    "if USE_IMAGE_FOR_Z:\n",
    "    # QRCode模式推荐prompts：有具体内容，但风格强调高对比度、黑白、简洁\n",
    "    # 参考论文中的例子：四张\"playground\"图片叠加后显示QRCode\n",
    "    # 这里我们使用不同的语义内容，但都强调高对比度、黑白风格\n",
    "    prompt_a = \"playground scene, high contrast, black and white photography, monochrome\"\n",
    "    prompt_b = \"urban building, high contrast, black and white, stark lighting\"\n",
    "    prompt_c = \"geometric architecture, high contrast, monochrome, simple composition\"\n",
    "    prompt_d = \"abstract landscape, high contrast, black and white, minimalist\"\n",
    "    prompt_z = \"QR code\"  # 这个不会被使用，但需要定义\n",
    "    print(\"⚠️ QRCode模式：已自动使用有语义内容但风格匹配QRCode的prompts\")\n",
    "    print(\"   每张图片都有独立的语义解释（保持illusion效果）\")\n",
    "    print(\"   但风格上强调高对比度、黑白，有助于overlay后显示QRCode\")\n",
    "    print(\"   如需自定义，可以修改prompt_a, prompt_b, prompt_c, prompt_d\")\n",
    "    print(\"   建议在prompts中添加：high contrast, black and white, monochrome 等关键词\")\n",
    "else:\n",
    "    # 默认模式：使用示例prompts\n",
    "    #These prompts are all strings - you can replace them with whatever you want! By default it lets you choose from example prompts\n",
    "    prompt_a, prompt_b, prompt_c, prompt_d, prompt_z = rp.gather(example_prompts, 'miku froggo lipstick kitten_in_box darth_vader'.split())\n",
    "    #Prompts a,b,c,d are the normal looking images\n",
    "    #Prompt z is the hidden image you get when you overlay them all on top of each other\n",
    "\n",
    "negative_prompt = ''\n",
    "\n",
    "print()\n",
    "print('Negative prompt:',repr(negative_prompt))\n",
    "print()\n",
    "print('Chosen prompts:')\n",
    "print('    prompt_a =', repr(prompt_a))\n",
    "print('    prompt_b =', repr(prompt_b))\n",
    "print('    prompt_c =', repr(prompt_c))\n",
    "print('    prompt_d =', repr(prompt_d))\n",
    "print('    prompt_z =', repr(prompt_z))\n",
    "print()\n",
    "if USE_IMAGE_FOR_Z:\n",
    "    print('⚠️ QRCode模式已启用：prompt_z将使用上传的图片作为目标')\n",
    "    print('⚠️ 已自动切换到有语义内容但风格匹配QRCode的prompts（a, b, c, d）')\n",
    "    print('   每张图片都有独立的语义解释，保持illusion效果')\n",
    "    print('   但风格上强调高对比度、黑白，有助于overlay后显示QRCode')\n",
    "    print('   如需自定义prompts，请在设置USE_IMAGE_FOR_Z之后修改prompt_a, prompt_b, prompt_c, prompt_d')\n",
    "    print('   建议在prompts中添加：high contrast, black and white, monochrome 等关键词')\n",
    "else:\n",
    "    print('ℹ️ 当前使用文本prompt模式。要启用QRCode模式，请设置 USE_IMAGE_FOR_Z = True')\n",
    "    print('   提示：QRCode模式下建议在prompts中添加\"high contrast, black and white\"等关键词')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0eh7vWFfPQ6"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85,
     "referenced_widgets": [
      "ec572d79f75648e69c18f8fbb8445abe",
      "8a95ebc987364a3cbbc27866ff8c964b",
      "13511603d504480489e9de201dd85211",
      "f72921cdc424455e8d2e86b0ea07a29b",
      "df5753e3d2b54078a5ff58f1f0f5de78",
      "f4b3425062c041b3aa5db68a73ea0a01",
      "eb7e46a3bff547ba8d07ebc87eddd9a0",
      "875b8337c4304ae0b9ea63eb7435d3ba",
      "f767f331f3af45d4a3814a5447d29004",
      "19d10e0e23fa460c8ba196c1691ecef3",
      "38124b85cb824de2a2640dfede710372"
     ]
    },
    "id": "wi9Y9Zp5ejSP",
    "outputId": "b79dad35-3726-4efb-f640-dfb3f27df788"
   },
   "outputs": [],
   "source": [
    "if 's' not in dir():\n",
    "    model_name=\"CompVis/stable-diffusion-v1-4\"\n",
    "    gpu='cuda:0'\n",
    "    s=sd.StableDiffusion(gpu,model_name)\n",
    "device=s.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HL_pjdcFekG6"
   },
   "outputs": [],
   "source": [
    "label_a = NegativeLabel(prompt_a,negative_prompt)\n",
    "label_b = NegativeLabel(prompt_b,negative_prompt)\n",
    "label_c = NegativeLabel(prompt_c,negative_prompt)\n",
    "label_d = NegativeLabel(prompt_d,negative_prompt)\n",
    "label_z = NegativeLabel(prompt_z,negative_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6LoGGFTkelJJ"
   },
   "outputs": [],
   "source": [
    "#Image Parametrization and Initialization (this section takes vram)\n",
    "\n",
    "#Select Learnable Image Size (this has big VRAM implications!):\n",
    "#Note: We use implicit neural representations for better image quality\n",
    "#They're previously used in our paper \"TRITON: Neural Neural Textures make Sim2Real Consistent\" (see tritonpaper.github.io)\n",
    "# ... and that representation is based on Fourier Feature Networks (see bmild.github.io/fourfeat)\n",
    "learnable_image_maker = lambda: LearnableImageFourier(height=256, width=256, hidden_dim=256, num_features=128).to(s.device); SIZE=256\n",
    "# learnable_image_maker = lambda: LearnableImageFourier(height=512,width=512,num_features=256,hidden_dim=256,scale=20).to(s.device);SIZE=512\n",
    "\n",
    "image_a=learnable_image_maker()\n",
    "image_b=learnable_image_maker()\n",
    "image_c=learnable_image_maker()\n",
    "image_d=learnable_image_maker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "44f6FT72ems4"
   },
   "outputs": [],
   "source": [
    "CLEAN_MODE = True # If it's False, we augment the images by randomly simulating how good a random printer might be when making the overlays...\n",
    "\n",
    "def simulate_overlay(a,b,c,d):\n",
    "    if CLEAN_MODE:\n",
    "        exp=1\n",
    "        brightness=3\n",
    "        black=0\n",
    "    else:\n",
    "        exp=rp.random_float(.5,1)\n",
    "        brightness=rp.random_float(1,5)\n",
    "        black=rp.random_float(0,.5)\n",
    "        bottom=rp.blend(bottom,black,rp.random_float())\n",
    "        top=rp.blend(top,black,rp.random_float())\n",
    "    return (a**exp * b**exp *c**exp * d**exp * brightness).clamp(0,99).tanh()\n",
    "\n",
    "learnable_image_a=lambda: image_a()\n",
    "learnable_image_b=lambda: image_b()\n",
    "learnable_image_c=lambda: image_c()\n",
    "learnable_image_d=lambda: image_d()\n",
    "learnable_image_z=lambda: simulate_overlay(image_a(), image_b(), image_c(), image_d())\n",
    "\n",
    "params=chain(\n",
    "    image_a.parameters(),\n",
    "    image_b.parameters(),\n",
    "    image_c.parameters(),\n",
    "    image_d.parameters(),\n",
    ")\n",
    "# QRCode模式：使用更小的学习率以确保稳定训练\n",
    "if USE_IMAGE_FOR_Z and 'target_image_z' in globals() and target_image_z is not None:\n",
    "    optim=torch.optim.SGD(params,lr=3e-5)  # QRCode模式使用更小的学习率\n",
    "    print(\"⚠️ QRCode模式：已降低学习率到3e-5，以提高训练稳定性\")\n",
    "else:\n",
    "    optim=torch.optim.SGD(params,lr=1e-4)  # 默认学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pKCQQg9teoMt"
   },
   "outputs": [],
   "source": [
    "labels=[label_a, label_b, label_c, label_d, label_z]\n",
    "learnable_images=[learnable_image_a,learnable_image_b,learnable_image_c,learnable_image_d,learnable_image_z]\n",
    "\n",
    "#The weight coefficients for each prompt. For example, if we have [1,1,1,1,5], then the hidden prompt (prompt_z) will be prioritized\n",
    "# QRCode模式：增加label_z的权重以确保QRCode效果更好\n",
    "if USE_IMAGE_FOR_Z and 'target_image_z' in globals() and target_image_z is not None:\n",
    "    weights=[1,1,1,1,10]  # QRCode模式下，大幅增加label_z的权重（从3增加到10）\n",
    "    print(\"⚠️ QRCode模式：已增加label_z权重为10，以更好地匹配QRCode\")\n",
    "    print(\"   这将使训练时更频繁地优化QRCode目标\")\n",
    "else:\n",
    "    weights=[1,1,1,1,1]  # 默认权重\n",
    "\n",
    "weights=rp.as_numpy_array(weights)\n",
    "weights=weights/weights.sum()\n",
    "weights=weights*len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tWpLV_Toertv"
   },
   "outputs": [],
   "source": [
    "#For saving a timelapse\n",
    "ims=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6-FlVZPes09"
   },
   "outputs": [],
   "source": [
    "def get_display_image():\n",
    "    return rp.tiled_images(\n",
    "        [\n",
    "            *[rp.as_numpy_image(image()) for image in learnable_images[:-1]],\n",
    "            rp.as_numpy_image(learnable_image_z()),\n",
    "        ],\n",
    "        length=len(learnable_images),\n",
    "        border_thickness=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 725
    },
    "id": "bB-Uv4Y5et8J",
    "outputId": "0cf6d601-40a6-4140-f0c5-e4720d389822"
   },
   "outputs": [],
   "source": [
    "NUM_ITER=10000\n",
    "\n",
    "#Set the minimum and maximum noise timesteps for the dream loss (aka score distillation loss)\n",
    "s.max_step=MAX_STEP=990\n",
    "s.min_step=MIN_STEP=10 \n",
    "\n",
    "display_eta=rp.eta(NUM_ITER, title='Status: ')\n",
    "\n",
    "DISPLAY_INTERVAL = 200\n",
    "\n",
    "print('Every %i iterations we display an image in the form [image_a, image_b, image_c, image_d, image_z] where'%DISPLAY_INTERVAL)\n",
    "print('    image_z = image_a * image_b * image_c * image_d')\n",
    "print()\n",
    "print('Interrupt the kernel at any time to return the currently displayed image')\n",
    "print('You can run this cell again to resume training later on')\n",
    "print()\n",
    "print('Please expect this to take hours to get good images (especially on the slower Colab GPU\\'s! The longer you wait the better they\\'ll be')\n",
    "\n",
    "try:\n",
    "    # QRCode模式：确保每次迭代都优化QRCode\n",
    "    qrcode_mode = (USE_IMAGE_FOR_Z and 'target_image_z' in globals() and target_image_z is not None)\n",
    "    \n",
    "    for iter_num in range(NUM_ITER):\n",
    "        display_eta(iter_num) #Print the remaining time\n",
    "\n",
    "        preds=[]\n",
    "        \n",
    "        # QRCode模式：改进的训练策略\n",
    "        # 每次迭代都优化label_z，然后随机优化其他labels\n",
    "        if qrcode_mode:\n",
    "            # 首先优化QRCode（label_z）- 每次迭代都执行\n",
    "            pred_image = learnable_image_z()[None]  # [1, 3, H, W]\n",
    "            target_resized = F.interpolate(target_image_z[None], size=(pred_image.shape[2], pred_image.shape[3]), mode='bilinear', align_corners=False)\n",
    "            \n",
    "            # 使用更强的L2 loss和L1 loss\n",
    "            l2_loss = ((pred_image - target_resized) ** 2).mean()\n",
    "            l1_loss = torch.abs(pred_image - target_resized).mean()\n",
    "            # 组合loss：L2为主，L1辅助，权重大幅增加\n",
    "            image_loss = (l2_loss * 15.0 + l1_loss * 3.0) * weights[4]  # weights[4]是label_z的权重\n",
    "            image_loss.backward(retain_graph=True)\n",
    "            \n",
    "            # 然后随机优化其他labels（a, b, c, d）\n",
    "            other_labels = list(zip([label_a, label_b, label_c, label_d], \n",
    "                                   [learnable_image_a, learnable_image_b, learnable_image_c, learnable_image_d],\n",
    "                                   [weights[0], weights[1], weights[2], weights[3]]))\n",
    "            # 每次迭代随机选择1-2个其他labels进行优化\n",
    "            num_other = np.random.randint(1, 3)  # 随机选择1或2个\n",
    "            selected = np.random.choice(len(other_labels), size=min(num_other, len(other_labels)), replace=False)\n",
    "            \n",
    "            for idx in selected:\n",
    "                label, learnable_image, weight = other_labels[idx]\n",
    "                pred=s.train_step(\n",
    "                    label.embedding,\n",
    "                    learnable_image()[None],\n",
    "                    noise_coef=.1*weight,guidance_scale=60,\n",
    "                )\n",
    "                preds+=list(pred)\n",
    "        else:\n",
    "            # 正常模式：随机选择label\n",
    "            label_items = list(zip(labels,learnable_images,weights))\n",
    "            for idx, (label,learnable_image,weight) in enumerate(rp.random_batch(label_items, batch_size=1)):\n",
    "                pred=s.train_step(\n",
    "                    label.embedding,\n",
    "                    learnable_image()[None],\n",
    "                    noise_coef=.1*weight,guidance_scale=60,\n",
    "                )\n",
    "                preds+=list(pred)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if iter_num and not iter_num%(DISPLAY_INTERVAL*50):\n",
    "                #Wipe the slate every 50 displays so they don't get cut off\n",
    "                from IPython.display import clear_output\n",
    "                clear_output()\n",
    "\n",
    "            if not iter_num%DISPLAY_INTERVAL:\n",
    "                im = get_display_image()\n",
    "                ims.append(im)\n",
    "                rp.display_image(im)\n",
    "\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "except KeyboardInterrupt:\n",
    "    print()\n",
    "    print('Interrupted early at iteration %i'%iter_num)\n",
    "    im = get_display_image()\n",
    "    ims.append(im)\n",
    "    rp.display_image(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwhrgJiie3mX"
   },
   "outputs": [],
   "source": [
    "print('Image A')\n",
    "rp.display_image(rp.as_numpy_image(learnable_image_a()))\n",
    "\n",
    "print('Image B')\n",
    "rp.display_image(rp.as_numpy_image(learnable_image_b()))\n",
    "\n",
    "print('Image C')\n",
    "rp.display_image(rp.as_numpy_image(learnable_image_c()))\n",
    "\n",
    "print('Image D')\n",
    "rp.display_image(rp.as_numpy_image(learnable_image_d()))\n",
    "\n",
    "print('Image Z')\n",
    "rp.display_image(rp.as_numpy_image(learnable_image_z()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4YJJw4dXe4JJ"
   },
   "outputs": [],
   "source": [
    "def save_run(name):\n",
    "    folder=\"untracked/hidden_character_runs/%s\"%name\n",
    "    if rp.path_exists(folder):\n",
    "        folder+='_%i'%time.time()\n",
    "    rp.make_directory(folder)\n",
    "    ims_names=['ims_%04i.png'%i for i in range(len(ims))]\n",
    "    with rp.SetCurrentDirectoryTemporarily(folder):\n",
    "        rp.save_images(ims,ims_names,show_progress=True)\n",
    "    print()\n",
    "    print('Saved timelapse to folder:',repr(folder))\n",
    "    \n",
    "save_run('untitled') #You can give it a good custom name if you want!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "13511603d504480489e9de201dd85211": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_875b8337c4304ae0b9ea63eb7435d3ba",
      "max": 23,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f767f331f3af45d4a3814a5447d29004",
      "value": 23
     }
    },
    "19d10e0e23fa460c8ba196c1691ecef3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "38124b85cb824de2a2640dfede710372": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "875b8337c4304ae0b9ea63eb7435d3ba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8a95ebc987364a3cbbc27866ff8c964b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f4b3425062c041b3aa5db68a73ea0a01",
      "placeholder": "​",
      "style": "IPY_MODEL_eb7e46a3bff547ba8d07ebc87eddd9a0",
      "value": "Fetching 23 files: 100%"
     }
    },
    "df5753e3d2b54078a5ff58f1f0f5de78": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eb7e46a3bff547ba8d07ebc87eddd9a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ec572d79f75648e69c18f8fbb8445abe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8a95ebc987364a3cbbc27866ff8c964b",
       "IPY_MODEL_13511603d504480489e9de201dd85211",
       "IPY_MODEL_f72921cdc424455e8d2e86b0ea07a29b"
      ],
      "layout": "IPY_MODEL_df5753e3d2b54078a5ff58f1f0f5de78"
     }
    },
    "f4b3425062c041b3aa5db68a73ea0a01": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f72921cdc424455e8d2e86b0ea07a29b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_19d10e0e23fa460c8ba196c1691ecef3",
      "placeholder": "​",
      "style": "IPY_MODEL_38124b85cb824de2a2640dfede710372",
      "value": " 23/23 [00:00&lt;00:00, 1628.58it/s]"
     }
    },
    "f767f331f3af45d4a3814a5447d29004": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
